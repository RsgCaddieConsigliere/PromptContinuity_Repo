# A Brief, Incomplete, and Somewhat Incorrect History of Language Models

> Adapted from [BrexHQ Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering)

This summary is useful for contextualizing the evolution of prompting strategies in modern AI systems. It supports AI SOP documentation, prompt lineage audit, and model rationale frameworks.

---

### Timeline Highlights

- **2018: BERT** â€” Bi-directional attention, foundational pre-training.
- **2019â€“2020: GPT-1/2** â€” Autoregressive transformers emerge.
- **T5 & BART** â€” Blend of BERT-like understanding and GPT-like generation.
- **GPT-3** â€” Emergence of prompt engineering (175B params).
- **RLHF** â€” Human feedback fine-tuning for alignment.
- **Instruction Tuning** â€” Models learn to follow direct instructions.
- **Chain-of-Thought Prompting** â€” Enables multi-step reasoning.
- **Claude, Gemini, GPT-4** â€” Multimodal inputs and context expansion.

ðŸ“Œ *Adapted under fair use for educational and AI system documentation purposes.*
